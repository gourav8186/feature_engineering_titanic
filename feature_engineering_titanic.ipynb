{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e793cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#LOAD DATASET\n",
    "filename = 'titanic.csv'\n",
    "df = pd.read_csv(filename)\n",
    "print(\"✅ Dataset Loaded Successfully!\")\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()\n",
    "print(\"\\nMissing Values Summary:\\n\", df.isnull().sum().sort_values(ascending=False).head(10))\n",
    "print(\"\\nData Types:\\n\", df.dtypes)\n",
    "\n",
    "#DATA CLEANING\n",
    "\n",
    "# Handle missing values\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute numeric columns with median\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "\n",
    "# Impute categorical columns with most frequent (mode)\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "\n",
    "# Remove duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "print(\"\\nDuplicates removed:\", duplicates)\n",
    "\n",
    "# Outlier detection and treatment (IQR method)\n",
    "def cap_outliers(series):\n",
    "    Q1, Q3 = series.quantile(0.25), series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "    return np.clip(series, lower, upper)\n",
    "\n",
    "for col in num_cols:\n",
    "    df[col] = cap_outliers(df[col])\n",
    "\n",
    "print(\"✅ Data Cleaning Complete\")\n",
    "print(\"Remaining Missing Values:\", df.isnull().sum().sum())\n",
    "\n",
    "\n",
    "#DATA INTEGRATION\n",
    "\n",
    "# Standardize column names\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "print(\"✅ Column names standardized\")\n",
    "\n",
    "#DATA TRANSFORMATION (UPDATED & FIXED)\n",
    "\n",
    "# Re-identify categorical and numeric columns (after cleaning)\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(\"Categorical columns before encoding:\", cat_cols)\n",
    "print(\"Numeric columns before scaling:\", num_cols)\n",
    "\n",
    "# Label encode simple (low-cardinality) categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in cat_cols:\n",
    "    try:\n",
    "        if df[col].nunique() <= 10:\n",
    "            df[col + \"_LE\"] = le.fit_transform(df[col].astype(str))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipped {col} due to error: {e}\")\n",
    "\n",
    "# One-hot encode remaining categorical columns safely\n",
    "multi_cat_cols = [col for col in cat_cols if df[col].nunique() > 10 and col in df.columns]\n",
    "if multi_cat_cols:\n",
    "    df = pd.get_dummies(df, columns=multi_cat_cols, drop_first=True)\n",
    "    print(\"One-hot encoded columns:\", multi_cat_cols)\n",
    "else:\n",
    "    print(\"No high-cardinality categorical columns for one-hot encoding.\")\n",
    "\n",
    "# Scale numerical columns\n",
    "scaler = StandardScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "# Log transform skewed numeric columns\n",
    "for col in num_cols:\n",
    "    if (df[col] > 0).all() and abs(df[col].skew()) > 1:\n",
    "        df[col + \"_log\"] = np.log1p(df[col])\n",
    "\n",
    "print(\"✅ Data Transformation Completed Successfully!\")\n",
    "print(\"New shape:\", df.shape)\n",
    "\n",
    "\n",
    "#DATA REDUCTION (FIXED)\n",
    "\n",
    "# Drop irrelevant ID or text-heavy columns\n",
    "id_cols = [c for c in df.columns if any(x in c.lower() for x in ['id', 'name', 'ticket', 'cabin'])]\n",
    "df.drop(columns=id_cols, inplace=True, errors='ignore')\n",
    "\n",
    "print(\"Dropped irrelevant columns (if present):\", id_cols)\n",
    "\n",
    "# Keep only numeric columns for correlation\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Correlation-based feature reduction\n",
    "corr_matrix = numeric_df.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "df.drop(columns=to_drop, inplace=True, errors='ignore')\n",
    "print(\"Dropped highly correlated features:\", to_drop)\n",
    "\n",
    "# Optional PCA (retain 95% variance)\n",
    "try:\n",
    "    pca = PCA(n_components=0.95, random_state=42)\n",
    "    pca_transformed = pca.fit_transform(numeric_df)\n",
    "    print(\"✅ PCA applied successfully.\")\n",
    "    print(\"Original numeric shape:\", numeric_df.shape)\n",
    "    print(\"Reduced shape after PCA:\", pca_transformed.shape)\n",
    "except Exception as e:\n",
    "    print(\"⚠️ PCA skipped due to:\", e)\n",
    "\n",
    "print(\"✅ Data Reduction Complete\")\n",
    "print(\"Final dataset shape:\", df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
